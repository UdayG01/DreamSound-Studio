{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Total loss: 0.035532508\n",
      "Style loss: 0.035532508\n",
      "Content loss: 0\n",
      "Error in style transfer: 'float' object cannot be interpreted as an integer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gupta\\AppData\\Local\\Temp\\ipykernel_20924\\2650153528.py:204: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  length=librosa.get_duration(filename=content_path) * sr\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 311\u001b[0m\n\u001b[0;32m    308\u001b[0m     transfer\u001b[38;5;241m.\u001b[39mplot_spectrograms(content_spec, style_spec, generated_spec)\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 311\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 304\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    301\u001b[0m transfer \u001b[38;5;241m=\u001b[39m NeuralAudioStyleTransfer()\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m# Perform style transfer\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m content_spec, style_spec, generated_spec \u001b[38;5;241m=\u001b[39m \u001b[43mtransfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransfer_style\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n\u001b[0;32m    308\u001b[0m transfer\u001b[38;5;241m.\u001b[39mplot_spectrograms(content_spec, style_spec, generated_spec)\n",
      "Cell \u001b[1;32mIn[2], line 200\u001b[0m, in \u001b[0;36mNeuralAudioStyleTransfer.transfer_style\u001b[1;34m(self, content_path, style_path, output_path)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# Inverse STFT with validation\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m     audio_generated \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mistft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstft_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m librosa\u001b[38;5;241m.\u001b[39mParameterError:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# If ISTFT fails, try with different parameters\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     audio_generated \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mistft(\n\u001b[0;32m    209\u001b[0m         stft_matrix,\n\u001b[0;32m    210\u001b[0m         hop_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhop_length,\n\u001b[0;32m    211\u001b[0m         win_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_fft\n\u001b[0;32m    212\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\gupta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\librosa\\core\\spectrum.py:545\u001b[0m, in \u001b[0;36mistft\u001b[1;34m(stft_matrix, hop_length, win_length, n_fft, window, center, dtype, length, out)\u001b[0m\n\u001b[0;32m    542\u001b[0m shape\u001b[38;5;241m.\u001b[39mappend(expected_signal_len)\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 545\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(out\u001b[38;5;241m.\u001b[39mshape, shape):\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch for provided output array out.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    549\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "class NeuralAudioStyleTransfer:\n",
    "    def __init__(self):\n",
    "        # Hyperparameters\n",
    "        self.n_fft = 2048\n",
    "        self.hop_length = 512\n",
    "        self.learning_rate = 0.02\n",
    "        self.style_weight = 1e-2\n",
    "        self.content_weight = 1e4\n",
    "        self.iterations = 100\n",
    "        \n",
    "        \n",
    "        # Style and content layer weights\n",
    "        self.style_layers = ['block1_conv1', 'block2_conv1', \n",
    "                           'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "        self.content_layers = ['block4_conv2']\n",
    "        \n",
    "        self.style_weight_per_layer = 1.0 / len(self.style_layers)\n",
    "\n",
    "        # Initialize VGG19 model\n",
    "        self.model = self.build_feature_extractor()\n",
    "        \n",
    "    def build_feature_extractor(self):\n",
    "        \"\"\"Create feature extractor model using VGG19\"\"\"\n",
    "        vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "        vgg.trainable = False\n",
    "        \n",
    "        style_outputs = [vgg.get_layer(name).output for name in self.style_layers]\n",
    "        content_outputs = [vgg.get_layer(name).output for name in self.content_layers]\n",
    "        \n",
    "        return tf.keras.Model(vgg.input, style_outputs + content_outputs)\n",
    "    \n",
    "    def process_audio(self, file_path):\n",
    "        \"\"\"Load and process audio to spectrogram\"\"\"\n",
    "        # Load audio\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        \n",
    "        # Compute spectrogram\n",
    "        stft = librosa.stft(audio, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "        spectrogram = np.abs(stft)\n",
    "        phase = np.angle(stft)\n",
    "        \n",
    "        # Convert to dB scale\n",
    "        spectrogram_db = librosa.amplitude_to_db(spectrogram)\n",
    "        \n",
    "        return spectrogram_db, phase, sr\n",
    "    \n",
    "    def prepare_spectrogram_for_vgg(self, spectrogram):\n",
    "        \"\"\"Convert spectrogram to VGG19 input format\"\"\"\n",
    "        # Normalize to [0, 1]\n",
    "        spec_norm = (spectrogram - np.min(spectrogram)) / (np.max(spectrogram) - np.min(spectrogram))\n",
    "        \n",
    "        # Convert to RGB by repeating channels\n",
    "        spec_rgb = np.stack([spec_norm] * 3, axis=-1)\n",
    "        \n",
    "        # Resize to VGG19 input size\n",
    "        spec_rgb = tf.image.resize(spec_rgb, (224, 224))\n",
    "        \n",
    "        # Add batch dimension and preprocess\n",
    "        spec_rgb = tf.keras.applications.vgg19.preprocess_input(spec_rgb[np.newaxis, ...])\n",
    "        \n",
    "        return spec_rgb\n",
    "    \n",
    "    def compute_gram_matrix(self, feature_maps):\n",
    "        \"\"\"Compute Gram matrix for style features\"\"\"\n",
    "        # Get shape using tf operations\n",
    "        shape = tf.shape(feature_maps)\n",
    "        height = shape[1]\n",
    "        width = shape[2]\n",
    "        channels = shape[3]\n",
    "        \n",
    "        # Reshape and compute gram matrix\n",
    "        features = tf.reshape(feature_maps, (height * width, channels))\n",
    "        gram = tf.matmul(features, features, transpose_a=True)\n",
    "        \n",
    "        # Normalize\n",
    "        normalizer = tf.cast(height * width * channels, tf.float32)\n",
    "        return gram / normalizer\n",
    "\n",
    "    \n",
    "    def compute_style_loss(self, style_targets, style_outputs):\n",
    "        \"\"\"Compute style loss using Gram matrices\"\"\"\n",
    "        style_loss = tf.zeros(shape=())\n",
    "        \n",
    "        for target, output in zip(style_targets, style_outputs):\n",
    "            gram_target = self.compute_gram_matrix(target)\n",
    "            gram_output = self.compute_gram_matrix(output)\n",
    "            layer_loss = tf.reduce_mean(tf.square(gram_output - gram_target))\n",
    "            style_loss += layer_loss * self.style_weight_per_layer\n",
    "            \n",
    "        return style_loss * self.style_weight\n",
    "\n",
    "    \n",
    "    def compute_content_loss(self, content_targets, content_outputs):\n",
    "        \"\"\"Compute content loss\"\"\"\n",
    "        content_loss = tf.zeros(shape=())\n",
    "        \n",
    "        for target, output in zip(content_targets, content_outputs):\n",
    "            content_loss += tf.reduce_mean(tf.square(output - target))\n",
    "            \n",
    "        return content_loss * self.content_weight\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, generated_spectrogram, style_targets, content_targets):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Get features\n",
    "            outputs = self.model(generated_spectrogram)\n",
    "            style_outputs = outputs[:len(self.style_layers)]\n",
    "            content_outputs = outputs[len(self.style_layers):]\n",
    "            \n",
    "            # Compute losses\n",
    "            style_loss = self.compute_style_loss(style_targets, style_outputs)\n",
    "            content_loss = self.compute_content_loss(content_targets, content_outputs)\n",
    "            total_loss = style_loss + content_loss\n",
    "            \n",
    "        # Compute gradients\n",
    "        grads = tape.gradient(total_loss, generated_spectrogram)\n",
    "        \n",
    "        # Update generated spectrogram using tf.assign\n",
    "        generated_spectrogram.assign_add(self.learning_rate * grads)\n",
    "        \n",
    "        return total_loss, style_loss, content_loss\n",
    "\n",
    "    \n",
    "    def transfer_style(self, content_path, style_path, output_path):\n",
    "        \"\"\"Main style transfer function\"\"\"\n",
    "        try:\n",
    "            # Process audio files\n",
    "            content_spec, content_phase, sr = self.process_audio(content_path)\n",
    "            style_spec, _, _ = self.process_audio(style_path)\n",
    "            \n",
    "            # Store original content spectrogram shape\n",
    "            original_shape = content_spec.shape\n",
    "            \n",
    "            # Prepare spectrograms for VGG19\n",
    "            content_input = self.prepare_spectrogram_for_vgg(content_spec)\n",
    "            style_input = self.prepare_spectrogram_for_vgg(style_spec)\n",
    "            \n",
    "            # Initialize generated spectrogram with content\n",
    "            generated_spectrogram = tf.Variable(content_input)\n",
    "            \n",
    "            # Extract style features\n",
    "            style_features = self.model(style_input)\n",
    "            style_targets = style_features[:len(self.style_layers)]\n",
    "            \n",
    "            # Extract content features\n",
    "            content_features = self.model(content_input)\n",
    "            content_targets = content_features[len(self.style_layers):]\n",
    "            \n",
    "            # Initialize best results\n",
    "            best_loss = float('inf')\n",
    "            best_spectrogram = None\n",
    "            \n",
    "            # Optimization loop\n",
    "            for i in range(self.iterations):\n",
    "                total_loss, style_loss, content_loss = self.train_step(\n",
    "                    generated_spectrogram, style_targets, content_targets)\n",
    "                \n",
    "                # Save best result\n",
    "                if total_loss < best_loss:\n",
    "                    best_loss = total_loss\n",
    "                    best_spectrogram = generated_spectrogram.numpy().copy()\n",
    "                \n",
    "                if i % 100 == 0:\n",
    "                    tf.print('Iteration:', i)\n",
    "                    tf.print('Total loss:', total_loss)\n",
    "                    tf.print('Style loss:', style_loss)\n",
    "                    tf.print('Content loss:', content_loss)\n",
    "            \n",
    "            # Use the best result for reconstruction\n",
    "            generated_spec = best_spectrogram[0]\n",
    "            \n",
    "            # Resize back to original dimensions\n",
    "            generated_spec = tf.image.resize(\n",
    "                generated_spec,\n",
    "                (original_shape[0], original_shape[1])\n",
    "            ).numpy()\n",
    "            \n",
    "            # Denormalize and handle potential infinities\n",
    "            generated_spec = generated_spec[:, :, 0]  # Take first channel\n",
    "            generated_spec = np.clip(generated_spec, np.min(content_spec), np.max(content_spec))\n",
    "            \n",
    "            # Convert to amplitude\n",
    "            generated_spec = librosa.db_to_amplitude(generated_spec)\n",
    "            generated_spec = np.clip(generated_spec, 0, np.max(generated_spec))\n",
    "            \n",
    "            # Combine with phase\n",
    "            stft_matrix = generated_spec * np.exp(1j * content_phase)\n",
    "            \n",
    "            # Inverse STFT with validation\n",
    "            try:\n",
    "                audio_generated = librosa.istft(\n",
    "                    stft_matrix,\n",
    "                    hop_length=self.hop_length,\n",
    "                    win_length=self.n_fft,\n",
    "                    length=librosa.get_duration(filename=content_path) * sr\n",
    "                )\n",
    "            except librosa.ParameterError:\n",
    "                # If ISTFT fails, try with different parameters\n",
    "                audio_generated = librosa.istft(\n",
    "                    stft_matrix,\n",
    "                    hop_length=self.hop_length,\n",
    "                    win_length=self.n_fft\n",
    "                )\n",
    "            \n",
    "            # Ensure finite values and normalize\n",
    "            audio_generated = np.nan_to_num(audio_generated, 0)\n",
    "            audio_generated = np.clip(audio_generated, -1, 1)\n",
    "            \n",
    "            # Apply smoothing\n",
    "            audio_smoothed = self.apply_smoothing(audio_generated, sr)\n",
    "            \n",
    "            # Final normalization and validation\n",
    "            audio_smoothed = np.nan_to_num(audio_smoothed, 0)\n",
    "            max_val = np.max(np.abs(audio_smoothed))\n",
    "            if max_val > 0:\n",
    "                audio_smoothed = audio_smoothed / max_val\n",
    "            audio_smoothed = np.clip(audio_smoothed, -1, 1)\n",
    "            \n",
    "            # Validate before saving\n",
    "            if not np.isfinite(audio_smoothed).all():\n",
    "                raise ValueError(\"Generated audio contains invalid values\")\n",
    "            \n",
    "            # Save output\n",
    "            sf.write(output_path, audio_smoothed, sr)\n",
    "            print(f\"Output saved to {output_path}\")\n",
    "            \n",
    "            # Compute spectrograms for visualization with validation\n",
    "            def safe_spectrogram(audio, sr):\n",
    "                spec = librosa.stft(audio, n_fft=self.n_fft)\n",
    "                mag = np.abs(spec)\n",
    "                mag = np.clip(mag, np.finfo(float).eps, None)  # Avoid log of zero\n",
    "                return librosa.amplitude_to_db(mag)\n",
    "            \n",
    "            # Load original audio for visualization\n",
    "            content_audio, _ = librosa.load(content_path, sr=sr)\n",
    "            style_audio, _ = librosa.load(style_path, sr=sr)\n",
    "            \n",
    "            return (\n",
    "                safe_spectrogram(content_audio, sr),\n",
    "                safe_spectrogram(style_audio, sr),\n",
    "                safe_spectrogram(audio_smoothed, sr)\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in style transfer: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def apply_smoothing(self, audio, sr, cutoff_freq=8000, order=5):\n",
    "        \"\"\"Apply low-pass filter to smooth the audio with validation\"\"\"\n",
    "        # Ensure finite values\n",
    "        audio = np.nan_to_num(audio, 0)\n",
    "        \n",
    "        # Apply filter\n",
    "        nyquist = 0.5 * sr\n",
    "        normalized_cutoff = cutoff_freq / nyquist\n",
    "        b, a = butter(order, normalized_cutoff, btype='low', analog=False)\n",
    "        \n",
    "        # Filter and validate\n",
    "        smoothed = filtfilt(b, a, audio)\n",
    "        smoothed = np.nan_to_num(smoothed, 0)\n",
    "        smoothed = np.clip(smoothed, -1, 1)\n",
    "        \n",
    "        return smoothed\n",
    "\n",
    "    \n",
    "    def plot_spectrograms(self, content_spec, style_spec, generated_spec):\n",
    "        \"\"\"Visualize the spectrograms\"\"\"\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        ax1.imshow(content_spec)\n",
    "        ax1.set_title('Content')\n",
    "        \n",
    "        ax2.imshow(style_spec)\n",
    "        ax2.set_title('Style')\n",
    "        \n",
    "        ax3.imshow(generated_spec)\n",
    "        ax3.set_title('Generated')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    # Set file paths\n",
    "    content_path = \"../audio/fade.mp3\"\n",
    "    style_path = \"../audio/dont.mp3\"\n",
    "    output_path = \"../audio/output_neural_style.wav\"\n",
    "    \n",
    "    # Create style transfer object\n",
    "    transfer = NeuralAudioStyleTransfer()\n",
    "    \n",
    "    # Perform style transfer\n",
    "    content_spec, style_spec, generated_spec = transfer.transfer_style(\n",
    "        content_path, style_path, output_path)\n",
    "    \n",
    "    # Plot results\n",
    "    transfer.plot_spectrograms(content_spec, style_spec, generated_spec)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
