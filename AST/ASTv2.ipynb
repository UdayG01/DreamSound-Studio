{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoWBe2SRRMKZ",
        "outputId": "cfbfc53f-77d3-4a72-c7e5-368f931eac36"
      },
      "outputs": [],
      "source": [
        "from scipy.signal import butter, filtfilt\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf1\n",
        "tf1.disable_v2_behavior()  # We need this for inception model compatibility\n",
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import os   \n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "import requests, zipfile\n",
        "import PIL.Image\n",
        "from IPython.display import display, Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_gj4ETYNS3E5"
      },
      "outputs": [],
      "source": [
        "def apply_low_pass_filter(audio, sample_rate, cutoff_frequency=8000, order=5):\n",
        "    \"\"\"\n",
        "    Applies a low-pass Butterworth filter to smooth the audio signal.\n",
        "    \"\"\"\n",
        "    nyquist = 0.5 * sample_rate\n",
        "    normalized_cutoff = cutoff_frequency / nyquist\n",
        "    b, a = butter(order, normalized_cutoff, btype='low', analog=False)\n",
        "    smoothed_audio = filtfilt(b, a, audio)\n",
        "    return smoothed_audio\n",
        "\n",
        "# Define paths\n",
        "content_audio_path = \"../audio/dont.mp3\"\n",
        "style_audio_path = \"../audio/fade.mp3\"\n",
        "output_audio_path = \"../audio/outdont2.wav\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "43I3Gr42S794"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "N_FFT = 2048  # Changed to match inception input requirements better\n",
        "N_FILTERS = 4096\n",
        "ALPHA = 1e-3  # Content loss weight\n",
        "BETA = 1.0    # Style loss weight\n",
        "learning_rate = 1e-4\n",
        "iterations = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "n9IE8GhVRh16"
      },
      "outputs": [],
      "source": [
        "def download_inception_model():\n",
        "    url = \"https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip\"\n",
        "    local_zip_file = \"inception5h.zip\"\n",
        "\n",
        "    if not os.path.exists('tensorflow_inception_graph.pb'):\n",
        "        print(\"Downloading Inception model...\")\n",
        "        response = requests.get(url)\n",
        "        with open(local_zip_file, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        with zipfile.ZipFile(local_zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall()\n",
        "\n",
        "        os.remove(local_zip_file)\n",
        "        print(\"Inception model downloaded and extracted.\")\n",
        "\n",
        "def load_inception_model():\n",
        "    model_fn = 'tensorflow_inception_graph.pb'\n",
        "    graph = tf1.Graph()\n",
        "    sess = tf1.InteractiveSession(graph=graph)\n",
        "    with tf.io.gfile.GFile(model_fn, 'rb') as f:\n",
        "        graph_def = tf1.GraphDef()\n",
        "        graph_def.ParseFromString(f.read())\n",
        "    t_input = tf1.placeholder(np.float32, name='input')\n",
        "    imagenet_mean = 117.0\n",
        "    t_preprocessed = tf1.expand_dims(t_input-imagenet_mean, 0)\n",
        "    tf1.import_graph_def(graph_def, {'input': t_preprocessed})\n",
        "    return graph, sess, t_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kg5-tkSSSM0G"
      },
      "outputs": [],
      "source": [
        "def T(layer):\n",
        "    '''Helper for getting layer output tensor'''\n",
        "    return graph.get_tensor_by_name(f\"import/{layer}:0\")\n",
        "\n",
        "def read_audio_spectrum(filename):\n",
        "    \"\"\"Read and process audio file into spectrum\"\"\"\n",
        "    x, fs = librosa.load(filename, sr=None)\n",
        "    print(f\"Sampling rate: {fs}\")\n",
        "\n",
        "    # Compute STFT\n",
        "    S = librosa.stft(x, n_fft=N_FFT)\n",
        "    p = np.angle(S)\n",
        "\n",
        "    # Take log of magnitude\n",
        "    S = np.log1p(np.abs(S[:,:430]))\n",
        "    \n",
        "    # Normalize to [0, 1] range\n",
        "    S = S / np.max(S)\n",
        "    \n",
        "    # Create 3-channel representation (RGB-like)\n",
        "    S_3channel = np.stack([S] * 3, axis=-1)  # Shape becomes [height, width, 3]\n",
        "    \n",
        "    return S_3channel, fs, p, x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "eNJ0o6QPRm5-"
      },
      "outputs": [],
      "source": [
        "def build_model(x):\n",
        "    \"\"\"Extract features using inception model\"\"\"\n",
        "    if isinstance(x, np.ndarray):\n",
        "        x = tf1.convert_to_tensor(x, dtype=tf1.float32)\n",
        "    \n",
        "    # Define layer\n",
        "    content_layer = 'mixed3b_1x1_pre_relu'\n",
        "    \n",
        "    # Get the tensor for the layer\n",
        "    layer_tensor = T(content_layer)\n",
        "    \n",
        "    with graph.as_default():\n",
        "        if isinstance(x, tf.Tensor):\n",
        "            features = sess.run(layer_tensor, {t_input: sess.run(x)})\n",
        "        else:\n",
        "            features = sess.run(layer_tensor, {t_input: x})\n",
        "    \n",
        "    return features\n",
        "\n",
        "def compute_gram_matrix(features):\n",
        "    \"\"\"Compute gram matrix from features\"\"\"\n",
        "    # Get shape information\n",
        "    shape = tf1.shape(features)\n",
        "    \n",
        "    # Reshape features\n",
        "    features_reshaped = tf1.reshape(features, (-1, tf1.shape(features)[-1]))\n",
        "    # Compute gram matrix\n",
        "    gram = tf1.matmul(tf1.transpose(features_reshaped), features_reshaped)\n",
        "    # Normalize\n",
        "    return gram / tf1.cast(tf1.shape(features_reshaped)[0], tf.float32)\n",
        "\n",
        "def compute_content_loss(content_features, gen_features):\n",
        "    \"\"\"Compute content loss\"\"\"\n",
        "    return ALPHA * tf1.reduce_mean(tf1.square(gen_features - content_features))\n",
        "\n",
        "def compute_style_loss(style_gram, gen_features):\n",
        "    \"\"\"Compute style loss\"\"\"\n",
        "    gen_gram = compute_gram_matrix(gen_features)\n",
        "    return BETA * tf1.reduce_mean(tf1.square(gen_gram - style_gram))\n",
        "\n",
        "def train_step(x_gen, content_features, style_gram):\n",
        "    with graph.as_default():\n",
        "        # Create the forward pass operations in the graph\n",
        "        gen_features_tensor = T('mixed3b_1x1_pre_relu')\n",
        "        \n",
        "        # Compute features and losses within the graph, not with sess.run\n",
        "        gen_features = gen_features_tensor\n",
        "        \n",
        "        # Compute losses (all operations should be in the graph)\n",
        "        content_loss = ALPHA * tf1.reduce_mean(tf1.square(gen_features - content_features))\n",
        "        \n",
        "        # Compute gram matrix and style loss\n",
        "        shape = tf1.shape(gen_features)\n",
        "        features_reshaped = tf1.reshape(gen_features, (-1, shape[-1]))\n",
        "        gram = tf1.matmul(tf1.transpose(features_reshaped), features_reshaped)\n",
        "        gram = gram / tf1.cast(tf1.shape(features_reshaped)[0], tf.float32)\n",
        "        \n",
        "        style_loss = BETA * tf1.reduce_mean(tf1.square(gram - style_gram))\n",
        "        \n",
        "        # Total loss\n",
        "        total_loss = content_loss + style_loss\n",
        "        \n",
        "        # Compute gradients\n",
        "        gradients = tf1.gradients(total_loss, [x_gen])[0]\n",
        "        \n",
        "        if gradients is None:\n",
        "            raise ValueError(\"Gradients are None. Check the computation graph connections.\")\n",
        "        \n",
        "        # Create update operation\n",
        "        update_op = x_gen.assign_sub(learning_rate * gradients)\n",
        "        \n",
        "        # Run the update and get loss values\n",
        "        _, total_loss_val, content_loss_val, style_loss_val = sess.run(\n",
        "            [update_op, total_loss, content_loss, style_loss],\n",
        "            feed_dict={t_input: sess.run(x_gen)}  # Provide the current value of x_gen\n",
        "        )\n",
        "        \n",
        "        return total_loss_val, content_loss_val, style_loss_val\n",
        "\n",
        "\n",
        "# Main execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "G4smWVBuTnYS"
      },
      "outputs": [],
      "source": [
        "def plot_spectrograms(content_spec, style_spec, output_spec):\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    ax1.imshow(content_spec, aspect='auto')\n",
        "    ax1.set_title('Content Audio')\n",
        "\n",
        "    ax2.imshow(style_spec, aspect='auto')\n",
        "    ax2.set_title('Style Audio')\n",
        "\n",
        "    ax3.imshow(output_spec, aspect='auto')\n",
        "    ax3.set_title('Output Audio')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6sGDDAiRrWf",
        "outputId": "9f3f0bfa-7af9-4505-d5e9-5a6557fdb41f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERROR:tensorflow:An interactive session is already active. This can cause out-of-memory errors or some other unexpected errors (due to the unpredictable timing of garbage collection) in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s). Please use `tf.Session()` if you intend to productionize.\n"
          ]
        }
      ],
      "source": [
        "# Download and load inception model\n",
        "download_inception_model()\n",
        "graph, sess, t_input = load_inception_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "GFWwznPNT3k_",
        "outputId": "b5e10eff-53af-478c-c687-0dd5741faa5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing content audio...\n",
            "Sampling rate: 44100\n",
            "Processing style audio...\n",
            "Sampling rate: 44100\n",
            "Extracting features...\n",
            "Starting style transfer...\n",
            "Error in iteration 0: Gradients are None. Check the computation graph connections.\n",
            "Processing final result...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "could not broadcast input array from shape (3,430) into shape (1025,430,3)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m result \u001b[38;5;241m=\u001b[39m sess\u001b[38;5;241m.\u001b[39mrun(x_gen)\n\u001b[0;32m     60\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(content_spectrum)\n\u001b[1;32m---> 61\u001b[0m \u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mN_CHANNELS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(result[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Phase reconstruction\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerforming phase reconstruction...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (3,430) into shape (1025,430,3)"
          ]
        }
      ],
      "source": [
        "# Load and process audio files\n",
        "print(\"Processing content audio...\")\n",
        "content_spectrum, content_fs, content_phase, content_audio = read_audio_spectrum(content_audio_path)\n",
        "print(\"Processing style audio...\")\n",
        "style_spectrum, style_fs, _, _ = read_audio_spectrum(style_audio_path)\n",
        "\n",
        "# Adjust style spectrum to match content dimensions\n",
        "N_SAMPLES = content_spectrum.shape[1]\n",
        "N_CHANNELS = content_spectrum.shape[0]\n",
        "style_spectrum = style_spectrum[:N_CHANNELS, :N_SAMPLES]\n",
        "\n",
        "    \n",
        "# Main execution\n",
        "with graph.as_default():\n",
        "    # Prepare inputs (make sure these are numpy arrays)\n",
        "    content_tf = np.expand_dims(content_spectrum, 0).astype(np.float32)\n",
        "    style_tf = np.expand_dims(style_spectrum, 0).astype(np.float32)\n",
        "    \n",
        "    # Extract initial features\n",
        "    print(\"Extracting features...\")\n",
        "    content_features = build_model(content_tf)\n",
        "    style_features = build_model(style_tf)\n",
        "    \n",
        "    # Compute style gram matrix\n",
        "    style_features_reshaped = np.reshape(style_features, (-1, style_features.shape[-1]))\n",
        "    style_gram = np.matmul(style_features_reshaped.T, style_features_reshaped) / style_features_reshaped.shape[0]\n",
        "    \n",
        "    # Convert to TF tensors\n",
        "    content_features = tf1.convert_to_tensor(content_features)\n",
        "    style_gram = tf1.convert_to_tensor(style_gram)\n",
        "    \n",
        "    # Initialize generated audio\n",
        "    x_gen = tf1.get_variable('generated', \n",
        "        shape=[1, content_tf.shape[1], content_tf.shape[2], 3],\n",
        "        initializer=tf1.random_normal_initializer(stddev=1e-3))\n",
        "    \n",
        "    # Initialize all variables\n",
        "    sess.run(tf1.global_variables_initializer())\n",
        "    \n",
        "    # Training loop\n",
        "    print(\"Starting style transfer...\")\n",
        "    for i in range(iterations):\n",
        "        try:\n",
        "            total_loss, content_loss, style_loss = train_step(x_gen, content_features, style_gram)\n",
        "            \n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f'Iteration {i + 1}, '\n",
        "                      f'Total Loss: {total_loss:.4f}, '\n",
        "                      f'Content Loss: {content_loss:.4f}, '\n",
        "                      f'Style Loss: {style_loss:.4f}')\n",
        "        except Exception as e:\n",
        "            print(f\"Error in iteration {i}: {str(e)}\")\n",
        "            break\n",
        "\n",
        "# Process result\n",
        "print(\"Processing final result...\")\n",
        "\n",
        "result = sess.run(x_gen)\n",
        "\n",
        "a = np.zeros_like(content_spectrum)\n",
        "a[:N_CHANNELS,:] = np.exp(result[0,0].T) - 1\n",
        "\n",
        "# Phase reconstruction\n",
        "print(\"Performing phase reconstruction...\")\n",
        "p = 2 * np.pi * np.random.random_sample(a.shape) - np.pi\n",
        "for i in range(500):\n",
        "    S = a * np.exp(1j*p)\n",
        "    x = librosa.istft(S, n_fft=N_FFT)\n",
        "    p = np.angle(librosa.stft(x, n_fft=N_FFT))\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Phase reconstruction iteration {i}\")\n",
        "\n",
        "# Apply low-pass filter\n",
        "print(\"Applying final processing...\")\n",
        "x_rec_smoothed = apply_low_pass_filter(x, content_fs)\n",
        "\n",
        "# Save result\n",
        "sf.write(output_audio_path, x_rec_smoothed, content_fs)\n",
        "print(f\"Output audio saved to {output_audio_path}\")\n",
        "\n",
        "# Plot spectrograms\n",
        "print(\"Generating spectrograms...\")\n",
        "output_spectrum, _ = read_audio_spectrum(output_audio_path)\n",
        "plot_spectrograms(content_spectrum, style_spectrum, output_spectrum)\n",
        "\n",
        "# Display audio players\n",
        "from IPython.display import Audio\n",
        "print(\"\\nContent Audio:\")\n",
        "display(Audio(content_audio_path))\n",
        "print(\"\\nStyle Audio:\")\n",
        "display(Audio(style_audio_path))\n",
        "print(\"\\nStyled Output:\")\n",
        "display(Audio(output_audio_path))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
